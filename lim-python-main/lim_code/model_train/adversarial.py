import numpy as np
import cvxpy as cp

from lim_code.defense_type_enum import DefenseType
from lim_code.knowledge_scenario_enum import KnowledgeScenario
from lim_code.model_train.safew import safew

def is_poisoned(
        baseline_model,
        base_models,
        X_malicious,
        poisoned_weights,
):
    poisoned_pred, _ = safew(
        baseline_model=baseline_model,
        base_models=base_models,
        X=X_malicious,
        weights=poisoned_weights)

    return poisoned_pred[0][0] == -1


def poison_weights(
        defense_type,
        knowledge_scenario,
        base_models,
        cloud_weights,
        compromised_weights,
        honest_client_weights,
        X_malicious):

    def create_targeted_goal_vector():
        # create perturbation vector such that the allied learners sum to 1.
        wrong_models = [model.predict(X_malicious)[0] == -1 for model in base_models]
        g_vector = np.array(wrong_models, dtype='f')
        g_vector = g_vector / g_vector.sum()
        return g_vector

    oracle_dict = {}

    ###############################
    # Question 1: Type of attack? #
    ###############################
    goal_vector = create_targeted_goal_vector()

    #####################################################
    # Question 2: Knowledge of honest clients' weights? #
    #####################################################
    if knowledge_scenario == KnowledgeScenario.AGR_UPDATES or \
        knowledge_scenario == KnowledgeScenario.UPDATES_ONLY:
        benign_ref_weights = honest_client_weights.copy()
    else:
        benign_ref_weights = compromised_weights.copy()

    # construct the benign_vector (benign reference aggregate)
    benign_ref_weights = np.array(benign_ref_weights)
    benign_vector = np.mean(benign_ref_weights, axis=0)

    #############################
    # Question 3: AGR-tailored? #
    #############################
    if knowledge_scenario == KnowledgeScenario.AGR_UPDATES or \
        knowledge_scenario == KnowledgeScenario.AGR_ONLY:

        benign_vector = cloud_weights.copy()

        # What is the specific AGR?
        if defense_type == DefenseType.DISCARD_IRREGULAR:
            oracle_function = O_for_AGR_tailored_attack_discard_irregular
            oracle_dict["upper_bound"] = (cloud_weights + np.ones_like(cloud_weights)) / 2
            oracle_dict["lower_bound"] = (cloud_weights + np.zeros_like(cloud_weights)) / 2
        else:
            oracle_function = O_for_AGR_tailored_attack_no_defense
    else:

        oracle_function = O_for_AGR_agnostic_attack

        # calculate the max_distance between any two benign weights
        distances = []
        for update in benign_ref_weights:
            distance = np.linalg.norm((benign_ref_weights - update), axis=1) ** 2
            distances = distance[None, :] if not len(distances) else np.concatenate((distances, distance[None, :]), 0)

        max_distance = np.max(distances)

        oracle_dict["max_distance"] = max_distance.copy()
        oracle_dict["benign_ref_weights"] = benign_ref_weights.copy()

    ##############################
    # Construct malicious update #
    ##############################
    oracle_dict["benign_vector"] = benign_vector.copy()
    oracle_dict["goal_vector"] = goal_vector.copy()

    gamma = solve_for_gamma(oracle_function, oracle_dict)

    # construct the malicious vector using gamma
    malicious_update = (benign_vector + gamma * goal_vector)
    malicious_update = malicious_update / malicious_update.sum()

    return malicious_update


def solve_for_gamma(oracle_function, oracle_dict):

    gamma_init = np.array([1000.0], dtype='f')
    threshold_diff = 1e-5
    gamma_succ = 0
    step = gamma_init
    gamma = gamma_init

    while np.abs(gamma_succ - gamma) > threshold_diff:

        if oracle_function(gamma, oracle_dict):
            gamma_succ = gamma
            gamma = gamma + step / 2
        else:
            gamma = gamma - step / 2

        step = step / 2

    return gamma


def O_for_AGR_tailored_attack_no_defense(gamma, oracle_dict):
    """
    Strength of AGR tailored attack is upperbounded by the aggregation algorithm,
    which is known by the adversary. Because LiM's aggregation algorithm (i.e., mean)
    does not trim/remove any update, gamma can be arbitrarily large.
    """

    return True


def O_for_AGR_tailored_attack_discard_irregular(gamma, oracle_dict):
    """
    DEFENSE IMPROVEMENT 1: discard any updates where one of the weights
    exceeds lower or upper bound.
    """

    # construct the malicious vector using gamma
    malicious_vector = (oracle_dict["benign_vector"] + gamma * oracle_dict["goal_vector"])
    malicious_vector = malicious_vector / malicious_vector.sum()

    # evaluate
    total_irregularities = np.sum(malicious_vector > oracle_dict["upper_bound"]) \
                           + np.sum(malicious_vector < oracle_dict["lower_bound"])

    return total_irregularities == 0


def O_for_AGR_agnostic_attack(gamma, oracle_dict):
    """
    Strength of AGR-agnostic attack is upperbounded such that it performs best in
    the presence of most SOTA robust AGRâ€™s.
    """

    # construct the malicious vector using gamma
    malicious_vector = (oracle_dict["benign_vector"] + gamma * oracle_dict["goal_vector"])
    malicious_vector = malicious_vector / malicious_vector.sum()

    # evaluate
    distance = np.linalg.norm((oracle_dict["benign_ref_weights"] - malicious_vector), axis=1) ** 2
    max_d = np.max(distance)

    return max_d <= oracle_dict["max_distance"]