import pickle
import itertools
import configparser
from pathlib import Path

import numpy as np
import pandas as pd
from scipy import sparse
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif

from lim_code.model_train.safew import (
    safew_base_predictions,
)
from lim_code.lim_logger import logger

config = configparser.ConfigParser()
config.read("lim_code/generate_dataset.conf")

FEATURES_NPZ = config.get("dataset", "features_npz")
LABELS_NPZ = config.get("dataset", "labels_npz")
FEATURE_NAMES_PICKLE = config.get("dataset", "feature_names_pickle")
APP_NAMES_PICKLE = config.get("dataset", "app_names_pickle")
FEATURE_SELECTION_TRAINING_SIZE = 0.5


class LiMData():
    def __init__(self,
                 features_npz=FEATURES_NPZ,
                 labels_npz=LABELS_NPZ,
                 feature_names_pickle=FEATURE_NAMES_PICKLE,
                 app_names_pickle=APP_NAMES_PICKLE,
                 unlabeled_data_proportion=0.9,
                 client_unlabeled_proportion=0.5,
                 top_k=10,
                 top_k_features=20,
                 random_state=None,
                 feature_selector=SelectKBest(chi2, k=100),
                 LiM_initialization=True):
        """
        unlabeled_data_proportion: proportion of data used for testing (including testing the clients)
        client_unlabeled_proportion: what percentage of testing data is only visible to clients
        top_k: number of apps in the popular dataset (per class)
        top_k_features: number of features used to generate the popular dataset

        Labels are raveled and normalized to {1, -1}
        """
        self.features_npz = features_npz
        self.feature_selector = feature_selector

        self.labels_npz = labels_npz
        self.feature_names_pickle = feature_names_pickle
        self.app_names_pickle = app_names_pickle
        self.unlabeled_data_proportion = unlabeled_data_proportion
        self.client_unlabeled_proportion = client_unlabeled_proportion
        self.top_k = top_k
        self.top_k_features = top_k_features
        self.random_state = random_state
        self.app_names = None
        self.X_preinstalled = None
        self.y_preinstalled = None

        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.client_X_test = None
        self.cloud_X_test = None
        self.client_y_test = None
        self.cloud_y_test = None

        self.X_malicious = None
        self.y_malicious = None
        self.N_malicious = None

        # if false, all clients are initialized with 35 apps
        # and deleting apps is also possible.
        self.use_LiM_initialization = LiM_initialization

    def initialized(self):
        # self.X_train is not None and self.X_test is not None self.y_train is not None and self.y_test is not None and self.client_X_test is not None and self.cloud_X_test is not None and self.client_y_test is not None and self.cloud_y_test is not None
        return self.cloud_y_test is not None

    def get(self):
        if not self.initialized():
            logger.info("Initializing the LiM data")
            self.initialize()
        return self

    def initialize(self):
        self.all_features = sparse.load_npz(self.features_npz)
        normal_apps_ind, preinstalled_ind = self.normal_preinstalled_ind(self.all_features)

        with Path(self.app_names_pickle).open(mode="rb") as f:
            app_names = pickle.load(f)
        self.N = app_names

        self.labels = self.labels()
        logger.info(f"Total clean apps = {np.sum(self.labels == -1)}")
        logger.info(f"Total malware apps = {np.sum(self.labels == 1)}")
        X_preinstalled = self.all_features.tocsr()[preinstalled_ind, :]
        self.y_preinstalled = self.labels[preinstalled_ind]

        self.N_preinstalled = list(itertools.compress(self.N, preinstalled_ind))

        logger.info(f"random state = {self.random_state}")
        X_train, X_test, self.y_train, self.y_test, self.N_train, self.N_test = train_test_split(
            self.all_features,
            self.labels,
            self.N,
            test_size=self.unlabeled_data_proportion,
            random_state=self.random_state
        )
        self.feature_selector = self.fit_selector()
        self.X_train, self.X_test, self.X_preinstalled = self.select_features(
            X_train, X_test,
            X_preinstalled,
        )
        self.client_X_test, cloud_X_test, self.client_y_test, cloud_y_test, self.client_N_test, cloud_N_test = train_test_split(
            self.X_test,
            self.y_test,
            self.N_test,
            test_size=self.client_unlabeled_proportion,
            random_state=self.random_state,
        )

        def share_testing_examples(X, y, N, n):
            max_index = X.shape[0]
            indices = np.random.randint(max_index, size=n)

            X_shared = X[indices, :]
            y_shared = y[indices]
            N_shared = list(itertools.compress(N, indices))
            return X_shared, y_shared, N_shared
        X_client, y_client, N_client = share_testing_examples(
            self.client_X_test,
            self.client_y_test,
            self.client_N_test,
            n=1000)
        self.cloud_X_test = sparse.vstack((cloud_X_test, X_client))
        self.cloud_y_test = np.concatenate((cloud_y_test, y_client))
        self.cloud_N_test = cloud_N_test + N_client

        # select 50 popular malicious apps
        self.client_X_test_popular_malicious = popular(
            X=self.client_X_test[self.client_y_test == 1],
            k_apps=self.top_k,
            k_features=self.top_k_features)

        # select 50 popular clean apps
        self.client_X_test_popular_clean = popular(
            X=self.client_X_test[self.client_y_test == -1],
            k_apps=self.top_k,
            k_features=self.top_k_features)

        # sort all installable client apps based on popularity
        self.client_X_test_sorted, self.client_y_test_sorted = popular3(
            X=self.client_X_test,
            y=self.client_y_test,
            k_features=self.top_k_features)

    def fit_selector(self):
        selector = self.feature_selector
        if self.all_features is None:
            self.all_features = sparse.load_npz(FEATURES_NPZ)
        if self.labels is None:
            self.labels = self.labels()

        X_train, _, y_train, _ = train_test_split(
            self.all_features, self.labels,
            test_size=1-FEATURE_SELECTION_TRAINING_SIZE,
            stratify=self.labels)
        selector.fit(X_train, y_train)

        return selector

    def labels(self):
        labels = sparse.load_npz(self.labels_npz)
        labels = labels.toarray().ravel()
        labels[labels != 1] = -1

        return labels

    def select_features(self, X_train, X_test, X_preinstalled):
        selector = self.feature_selector

        new_X_train = selector.transform(X_train)
        new_X_test = selector.transform(X_test)
        new_X_preinstalled = selector.transform(X_preinstalled)
        logger.info(f"From {X_train.shape[1]} to {new_X_train.shape[1]} dimensions using {selector}")

        return new_X_train, new_X_test, new_X_preinstalled

    def take_client_app(self, p_malware, p_popular=0, X=None, y=None, N=None):
        if X is None and y is None and N is None:
            X = self.client_X_test
            y = self.client_y_test
            N = self.client_N_test

        label = np.random.choice(
            [1, -1],
            p=[p_malware, 1-p_malware]
        )

        label_filter = y == label
        X_label = X[label_filter, :]
        N_label = list(itertools.compress(N, label_filter))
        random_popular = np.random.random()
        if random_popular < p_popular:
            X_label, N_label = popular(
                X=X_label,
                N=N_label,
                k_apps=self.top_k,
                k_features=self.top_k_features)

        random = np.random.randint(X_label.shape[0])

        app = X_label[random]
        name = N_label[random]

        return app, label, name

    def normal_preinstalled_ind(self, features):
        feature_names = load_feature_names(self.feature_names_pickle)
        preinstalled_column = features.getcol(feature_names.index("lim_preinstalled"))
        normal_apps_ind = (
            (preinstalled_column == 0)
            .toarray()
            .ravel()
        )
        preinstalled_ind = (
            (preinstalled_column == 1)
            .toarray()
            .ravel()
        )

        return normal_apps_ind, preinstalled_ind

    def select_malicious(self, baseline_model, base_models, weights):
        y = 1
        self.y_malicious = y
        malware = self.client_y_test == y
        X = self.client_X_test
        N = self.client_N_test

        X = X[malware, :]
        N = np.array(list(itertools.compress(self.client_N_test, malware)))

        permutation = np.random.permutation(X.shape[0])
        X = X[permutation, :]
        N = N[permutation]

        baseline_pred, candidate_pred = safew_base_predictions(
            baseline_model=baseline_model,
            base_models=base_models,
            X=X
        )

        def close_to_clean(candidates):
            allies = candidates == -1
            return 0.30 <= weights[allies].sum() <= 0.40
        
        for i, sample in enumerate(X):
            candidates = candidate_pred[i, :]
            if close_to_clean(candidates):
                allies = candidates == -1
                logger.info(f"Found malicious sample: allied sum = {weights[allies].sum()}")
                self.X_malicious = X[i, :].reshape(1, -1)  # Only one instance
                self.N_malicious = N[i]
                return


def popular(X, N=None, k_apps=None, k_features=None):
    features_popularity = X.sum(axis=0)  # shape 1,X.shape[0]
    sorted_features = np.argsort(features_popularity)  # shape 1,X.shape[0]
    top_features = sorted_features[0, -k_features:].tolist()[0]
    top_features.reverse()  # From the most popular to the less popular

    # Sort apps  by how many features they have from the top features
    # And take the first k_apps
    mask = np.array([False] * X.shape[1])
    mask[top_features] = True

    X_prevalence = X[:, mask].sum(axis=1).flatten().tolist()[0]
    sorted_X_prevalence = np.argsort(X_prevalence)[-k_apps:].tolist()
    sorted_X_prevalence.reverse()  # From the most popular to the less popular
    X_popular = X[sorted_X_prevalence, :]
    if N is None:
        return X_popular
    else:
        N_popular = list(itertools.compress(N, sorted_X_prevalence))
        return X_popular, N_popular


def popular3(X, y, k_features):
    features_popularity = X.sum(axis=0)  # shape 1,X.shape[0]
    sorted_features = np.argsort(features_popularity)  # shape 1,X.shape[0]
    top_features = sorted_features[0, -k_features:].tolist()[0]
    top_features.reverse()  # From the most popular to the less popular

    # Sort apps  by how many features they have from the top features
    # And take the first k_apps
    mask = np.array([False] * X.shape[1])
    mask[top_features] = True

    k_apps = len(y)

    X_prevalence = X[:, mask].sum(axis=1).flatten().tolist()[0]
    sorted_X_prevalence = np.argsort(X_prevalence)[-k_apps:].tolist()
    sorted_X_prevalence.reverse()  # From the most popular to the less popular
    X_popular = X[sorted_X_prevalence, :]
    y_popular = y[sorted_X_prevalence]

    return X_popular, y_popular


def load_feature_names(feature_names_pickle):
    with Path(feature_names_pickle).open(mode="rb") as f:
        feature_names = pickle.load(f)

    label = config.get("dataset", "label")
    feature_names.remove(label)
    return feature_names


def compare_features():
    """
    Generate files with chi2 and mutual information scores for every features.

    These files can be read using pd.read_csv to see the values and select the best K features. This is not automated.
    """

    selectors = ([
        SelectKBest(chi2, k=20),
        SelectKBest(mutual_info_classif, k=20)
    ])

    feature_names = load_feature_names(FEATURE_NAMES_PICKLE)
    all_features = sparse.load_npz(FEATURES_NPZ)
    labels = sparse.load_npz(LABELS_NPZ)
    labels = labels.toarray().ravel()
    labels[labels != 1] = -1
    X_train, X_test, y_train, y_test = train_test_split(
        all_features, labels, test_size=0.5, stratify=labels)

    for selector in selectors:
        selector_name = selector.score_func.__name__
        logger.info(f"Applying selector {selector_name}")
        new_features = selector.fit(X_train, y_train)

        df = pd.DataFrame.from_dict({"name": feature_names,
                                     "score": selector.scores_,
                                     "pvalue": selector.pvalues_})
        selector_name = selector.score_func.__name__
        df.to_csv(f"{selector_name}_features.csv")
        # Manual analysis to see scores and select the best K
